{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import ipywidgets as ipw\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive optimization\n",
    ".. codeauthor:: Emile Roux, Ludovic Charleux \n",
    "\n",
    "## Background\n",
    "\n",
    "The starting point, also known as the initial guess or initial solution, is crucial in optimization because it can greatly impact the outcome of the optimization process. \n",
    "In optimization, the goal is to find the optimal solution that maximizes or minimizes a certain objective function.\n",
    "If the initial guess is far from the optimal solution, it may take a long time for the algorithm to converge to the optimal solution, or it may converge to a suboptimal solution instead.\n",
    "Therefore, selecting a good starting point that is close to the optimal solution can greatly improve the efficiency and accuracy of the optimization process. \n",
    "The Goal of this notebook is to understand this keypoint of optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the cost function\n",
    "\n",
    "The 2D Himmelblau cost function is used to illustrate the starting point importantce. \n",
    "https://en.wikipedia.org/wiki/Himmelblau%27s_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def cost(p):\n",
    "    global counterCostCall\n",
    "    \"\"\"\n",
    "    Himmelblau cost function.\n",
    "    https://en.wikipedia.org/wiki/Himmelblau%27s_function\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = p\n",
    "    counterCostCall += 1\n",
    "    return (x**2 + y - 11) ** 2 + (x + y**2 - 7) ** 2\n",
    "\n",
    "\n",
    "counterCostCall = 0\n",
    "Nx, Ny = 100, 100\n",
    "x = np.linspace(-5.0, 5.0, Nx)\n",
    "y = np.linspace(-5.0, 5.0, Ny)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "zf = np.array([X.flatten(), Y.flatten()])\n",
    "Z = cost(zf).reshape(Nx, Ny)\n",
    "Z = np.where(Z > 200.0, np.nan, Z)\n",
    "\n",
    "plt.figure()\n",
    "title = plt.title(\"\")\n",
    "plt.contourf(X, Y, Z, 20, cmap=cm.jet)\n",
    "plt.colorbar()\n",
    "plt.contour(X, Y, Z, 20, cmap=cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This 2D function shows 4 different minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bluid a animated plot to have fun with starting points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "counterCostCall = 0\n",
    "P = np.zeros((1000, 2))\n",
    "P[0] = -1.0, 0.0\n",
    "\n",
    "\n",
    "def track(xk):\n",
    "    global counter, P\n",
    "    P[counter, :] = xk\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "title = plt.title(\"\")\n",
    "plt.contourf(X, Y, Z, 20, cmap=cm.jet)\n",
    "plt.colorbar()\n",
    "plt.contour(X, Y, Z, 20, cmap=cm.gray)\n",
    "(line,) = plt.plot(P[:, 0], P[:, 1], \"og-\", label=\"Path\")\n",
    "(line0,) = plt.plot(P[:1, 0], P[:1, 1], \"r*-\", label=\"Start\")\n",
    "(line1,) = plt.plot(\n",
    "    P[counter - 1 : counter, 0], P[counter - 1 : counter, 1], \"bs-\", label=\"End\"\n",
    ")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "@ipw.interact(\n",
    "    x0=(-4.0, 4, 0.1), y0=(-4.0, 4, 0.1), algorithm=[\"Nelder-Mead\", \"BFGS\", \"Powell\"]\n",
    ")\n",
    "def run_optimization(x0=0.0, y0=0.0, algorithm=\"Nelder-Mead\"):\n",
    "    global P, counter, counterCostCall\n",
    "    counter = 1\n",
    "    counterCostCall = 0\n",
    "    P *= np.nan\n",
    "    P[0] = x0, y0\n",
    "    sol = optimize.minimize(cost, [x0, y0], callback=track, method=algorithm)\n",
    "    line.set_xdata(P[:, 0])\n",
    "    line.set_ydata(P[:, 1])\n",
    "    line0.set_xdata(P[:1, 0])\n",
    "    line0.set_ydata(P[:1, 1])\n",
    "    line1.set_xdata(P[counter - 1 : counter, 0])\n",
    "    line1.set_ydata(P[counter - 1 : counter, 1])\n",
    "    title.set_text(\n",
    "        \"Alg:{0}, Iters: {1}, CostCall: {4}, sol=({2:.1f},{3:.1f})\".format(\n",
    "            algorithm, counter, sol.x[0], sol.x[1], counterCostCall\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the starting point and optimization method, we can observe:\n",
    "* The optimization process converged toward different minimum.\n",
    "* Number of iterations is not constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
