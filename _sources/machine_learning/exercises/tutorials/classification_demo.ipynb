{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ML:tutorial:classification_demo)=\n",
    "# Supervised learning using PyTorch: a toy example \n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this example, you will use a supervised learning strategy to train a neural network.\n",
    "The example is deliberately very simple to allow for great scalability and fast CPU learning. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from scipy import optimize\n",
    "import numba\n",
    "import torch\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"aircAruvnKk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact model\n",
    "\n",
    "In this first part, we propose to define an ideal classification model. It is a function which associates a value $z \\in \\left\\lbrace 0, 1 \\right \\rbrace$ to a tuple of values $(x, y)$. This function represents categories or classes that we would like to model the distribution in space. In a real case, this function would be inaccessible to us and we would try to find it through deep learning.\n",
    "\n",
    "We propose two examples of functions but you are encouraged to develop your own and to test them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_easy(inp):\n",
    "    \"\"\"\n",
    "    A ideal model.\n",
    "    \"\"\"\n",
    "    xc1, yc1 = 0.3, 0.6\n",
    "    xc2, yc2 = 0.55, 0.15\n",
    "    r1, r2 = 0.25, 0.15\n",
    "    X, Y = np.array(inp).T\n",
    "    R12 = (X - xc1) ** 2 + (Y - yc1) ** 2\n",
    "    R22 = (X - xc2) ** 2 + (Y - yc2) ** 2\n",
    "    return ((R22 <= r2**2) | (R12 <= r1**2)) * 1\n",
    "\n",
    "\n",
    "def func_hard(inp):\n",
    "    \"\"\"\n",
    "    Another ideal model, just a bit more complex.\n",
    "    \"\"\"\n",
    "    x, y = np.array(inp).T - 0.5\n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    theta = np.arccos(x / r) * np.sign(y)\n",
    "    out = (np.cos(3 * theta + 10 * r) >= 0.0) * 1\n",
    "    out[r <= 0.1] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "exact_model = func_easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we draw the model to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nxm, nym = 200, 200\n",
    "xm = np.linspace(0, 1.0, nxm)\n",
    "ym = np.linspace(0, 1.0, nym)\n",
    "Xm, Ym = np.meshgrid(xm, ym)\n",
    "pointsm = np.array([Xm.flatten(), Ym.flatten()]).T\n",
    "solm = exact_model(pointsm)\n",
    "solm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_aspect(\"equal\")\n",
    "plt.scatter(\n",
    "    pointsm[solm == 1].T[0], pointsm[solm == 1].T[1], s=1, color=\"white\", label=\"Ones\"\n",
    ")\n",
    "plt.scatter(\n",
    "    pointsm[solm == 0].T[0], pointsm[solm == 0].T[1], s=1, color=\"black\", label=\"Zeros\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning DB\n",
    "\n",
    "First, we will create a learning database. This one contains a certain number of points for which we know the class. The goal is to present them to our neural network so that it can learn to recognize the different classes. These points are randomly arranged in the space where the function is defined. We can vary the number of points and see how this affects the learning capacity of the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npl = 2000  # NUMBER OF SAMPLES\n",
    "pointsl = np.random.rand(npl * 2).reshape(npl, 2)\n",
    "soll = exact_model(pointsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_aspect(\"equal\")\n",
    "plt.scatter(\n",
    "    pointsl[soll == 1].T[0], pointsl[soll == 1].T[1], s=2, color=\"red\", label=\"Ones\"\n",
    ")\n",
    "plt.scatter(\n",
    "    pointsl[soll == 0].T[0], pointsl[soll == 0].T[1], s=2, color=\"blue\", label=\"Zeros\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network class\n",
    "\n",
    "A neural network is a stack of alternating layers. Linear layers where a linear (or affine) function is applied to the data and non-linear layers where a non-linear function or activation function (or neural function) is applied to the data. Building a network is therefore a matter of choosing the number of layers and the type of neural functions. The learning process only consists in optimizing the weights present in the linear layers. Neural functions do not usually have adjustable parameters.\n",
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)\n",
    "\n",
    "Evaluating the result of the network (inference) is the same as going from left to right, we speak of a forward path. On the contrary, to make it learn in a supervised approach, we have to analyze the effect of the different weights on the result and on the error it generates. This is called a backward path. \n",
    "\n",
    "You are asked to create your network and to test it.\n",
    "\n",
    "## Required work\n",
    "\n",
    "1. Try the code in the state it is in. Do you think it is effective?\n",
    "\n",
    "In particular, compare the level of error displayed during learning with the quality of the figure obtained at the end of the notebook. \n",
    "\n",
    "2. Try to improve it by playing on: the training dataset, the network structure. Draw conclusions.\n",
    "\n",
    "3. Try the different neural functions and see how they affect the behavior of the network.\n",
    "\n",
    "4. Try the `func_hard` function (see above), which has a more difficult structure to reproduce than the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "errors = []\n",
    "steps = []\n",
    "# BUILD LAYERS WITH:\n",
    "# LINEAR LAYERS:\n",
    "torch.nn.Linear(2, 4, bias=True)  # LINEAR LAYERS\n",
    "# NEURAL FUNCTIONS:\n",
    "torch.nn.ELU, torch.nn.ReLU, torch.nn.Hardtanh  # POSSIBLE ACTIVATION FUNCTIONS\n",
    "activation_func = torch.nn.ReLU\n",
    "\n",
    "# LAYERS INITIALIZATION\n",
    "layers = [\n",
    "    torch.nn.Linear(2, 4, bias=True),\n",
    "    activation_func(),\n",
    "    torch.nn.Linear(4, 1, bias=True),\n",
    "    torch.nn.Hardtanh(min_val=0.0, max_val=1.0),\n",
    "]\n",
    "model = torch.nn.Sequential(*layers).to(device)\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING INPUTS / OUTPUTS\n",
    "# NOTE: YOU CAN RERUN THIS CELL TO CONTINUE TRAINING PROCESS\n",
    "run_training = True  # SET TO True TO ACTUALLY RUN TRAINING\n",
    "if run_training:\n",
    "    x = torch.Tensor(pointsl).to(device)\n",
    "    t = torch.Tensor(soll[:, None]).to(device)\n",
    "    ns = x.shape[0]\n",
    "    learning_rate = 1e-3\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.MSELoss(reduction=\"sum\").to(device)\n",
    "    Ne = 10  # Number of training epoch\n",
    "    Nes = 100  # Number of training steps per epoch\n",
    "    error = np.zeros(Ne)\n",
    "    step = np.arange(Ne) * Nes\n",
    "    if len(steps) != 0:\n",
    "        step += steps[-1].max() + Nes\n",
    "    # TRAINING\n",
    "    for e in range(Ne):\n",
    "        for s in range(Nes):\n",
    "            y = model(x)\n",
    "            loss = loss_fn(y, t)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        error[e] = loss.item() / ns\n",
    "        err_str = \"{0:.3f}\".format(error[e] * 100).zfill(6)\n",
    "        print(f\"Loss = {err_str}\")\n",
    "    errors.append(error)\n",
    "    steps.append(step)\n",
    "\n",
    "# POST-PROCESSING\n",
    "if run_training:\n",
    "    errors2 = np.array(errors)\n",
    "    fig = plt.figure()\n",
    "    plt.title(\"Training process\")\n",
    "    # plt.yscale(\"log\")\n",
    "    for ep, err in enumerate(errors2):\n",
    "        plt.plot(steps[ep], err * 100, \"x-\", label=f\"Epoch {ep}\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss [%]\")\n",
    "    # plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_training:\n",
    "    yt = model(torch.Tensor(pointsm).to(device)).cpu().data.numpy().ravel()\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 3, 1)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    plt.title(\"Model\")\n",
    "    plt.tricontourf(pointsm.T[0], pointsm.T[1], solm, 2, cmap=mpl.cm.gray)\n",
    "    ax.axis(\"off\")\n",
    "    ax = fig.add_subplot(1, 3, 2)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    plt.title(\"Learned\")\n",
    "    plt.tricontourf(pointsm.T[0], pointsm.T[1], yt, cmap=mpl.cm.gray)\n",
    "    ax.axis(\"off\")\n",
    "    ax = fig.add_subplot(1, 3, 3)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    plt.title(\"Error\")\n",
    "    plt.tricontourf(pointsm.T[0], pointsm.T[1], yt - solm, cmap=mpl.cm.gray)\n",
    "    ax.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
